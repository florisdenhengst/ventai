{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fe4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "from collections.abc import Iterable\n",
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import numpy_ext as npe\n",
    "import math\n",
    "import random\n",
    "from pprint import pprint\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import poisson\n",
    "from scipy.sparse import hstack, vstack, csr_matrix\n",
    "import scipy\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from config import demographics, vital_sign_vars, lab_vars, treatment_vars, vent_vars, guideline_vars, ffill_windows_clinical, SAMPLE_TIME_H\n",
    "from config import fio2_bins, peep_bins, tv_bins\n",
    "import safety\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "494c748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "seed = 4\n",
    "\n",
    "SHAPING = 'none'\n",
    "COMPLIANCE_SCALAR = 0.0 # should be in [0, 5]\n",
    "UNSAFETY_PROB = 1.0 # should be in [0.0,1.0] or {0,0, 1.0} until safety probs implemented\n",
    "SOFTMAX_TEMPERATURE = 1.0 # \n",
    "GAMMA = 0.99\n",
    "N_EPOCHS = 10000\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "n_states = 650\n",
    "plot = False\n",
    "\n",
    "if SHAPING == 'none' and COMPLIANCE_SCALAR != 0.0:\n",
    "    raise Error('COMPLIANCE_SCALAR should be 0.0 if shaping approach is none')\n",
    "elif SHAPING != 'none' and COMPLIANCE_SCALAR <= 0:\n",
    "    raise Error('COMPLIANCE_SCALAR of {} not allowed for shaping approach {}, should be >0'.format(COMPLIANCE_SCALAR, SHAPING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a231b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO FdH: import trajectories -- do some simple checks\n",
    "test_set_file = 'data/test_unshaped_traj_{}.csv'\n",
    "train_set_file = 'data/train_unshaped_traj_{}.csv'\n",
    "\n",
    "train_set = pd.read_csv(train_set_file.format(seed))\n",
    "test_set = pd.read_csv(test_set_file.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a0f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compliance_to_potential(compliance):\n",
    "    return compliance * COMPLIANCE_SCALAR\n",
    "\n",
    "def potential_diff(x):\n",
    "    if np.isnan(x.iloc[1]):\n",
    "        p1 = 0.0 # see Grzes, AAMAS 2017\n",
    "    else:\n",
    "        p1 = GAMMA * x.iloc[1]\n",
    "    return p1 - x.iloc[0]\n",
    "\n",
    "# TODO FdH: reward shaping\n",
    "if SHAPING == 'avgpotential2' or SHAPING == 'avgbase':\n",
    "    train_set['compliance'] = safety.state_compliance_clinical(train_set, safety.avg_clinical_timestep)\n",
    "elif SHAPING == 'allpotential' or SHAPING == 'allbase':\n",
    "    train_set['compliance'] = safety.state_compliance_clinical(train_set, safety.all_clinical_timestep)\n",
    "elif SHAPING == 'none':\n",
    "    train_set['compliance'] = 0.0\n",
    "else:\n",
    "    raise ValueError('Unknown shaping approach')\n",
    "train_set['potential'] = compliance_to_potential(train_set['compliance'])\n",
    "if 'potential' in SHAPING:\n",
    "    train_set['shaping_reward_unshift'] = train_set.groupby('icustay_id').rolling(window=2)['potential'].apply(potential_diff).fillna(0.0).reset_index().set_index('level_1')['potential']\n",
    "    train_set['shaping_reward'] = train_set['shaping_reward_unshift'].shift(-1)\n",
    "    train_set.loc[train_set.terminal, 'shaping_reward'] = train_set['potential']\n",
    "elif 'base' in SHAPING:\n",
    "    train_set['shaping_reward'] = train_set['potential']\n",
    "elif SHAPING == 'none':\n",
    "    train_set['shaping_reward'] = 0.0\n",
    "\n",
    "train_set['reward'] = train_set.reward + train_set.shaping_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f751216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the tabular FQI algorithm in Ernst, Geurts & Wehenkel (2005), Figure 1\n",
    "# and Peine's supplementary discussion \"A: Evaluation of Policies\".\n",
    "def peine_mc_iterate(snsasr, Qn, gamma, n_epochs=1, learning_rate=0.1, unsafety_prob=0.0, safety_map=safety.action_id_compliance):\n",
    "    \"\"\"\n",
    "    Monte-carlo-based iteration of the training procedure according to tabular FQI & Peine's supplementary discussion.\n",
    "    \n",
    "    snsas: numpy ndarray with discretized state-nextstate-action tuples\n",
    "    r: a function that returns the immediate reward for a state-action pair\n",
    "    Qn: dictionary that maps iteration indices to Qn-estimates\n",
    "    n: iteration number\n",
    "    gamma: discount factor\n",
    "    n_epochs: number of times to iterate over dataset\n",
    "    learning rate: learning rate alpha\n",
    "    \"\"\"\n",
    "    def epoch(snsasr, Qn, gamma, learning_rate, unsafety_prob, safety_map):\n",
    "        for i, (s, ns, a, er) in enumerate(snsasr):\n",
    "            if unsafety_prob == 1.0:\n",
    "                # We do not care about the safety rules\n",
    "                Qn[s,a] = Qn[s,a] + learning_rate * (er + gamma * np.max(Qn[int(ns),:]) - Qn[s,a])\n",
    "            elif unsafety_prob == 0.0:\n",
    "                if safety_map[a]:\n",
    "                    Qn[s,a] = Qn[s,a] + learning_rate * (er + gamma * np.max(Qn[int(ns), safety_map]) - Qn[s,a])\n",
    "                else:\n",
    "                    # taken action not safe, disregard sample\n",
    "                    pass\n",
    "            else:\n",
    "                raise ValueError(\"Only unsafety probs in {0.0, 1.0} supported for now\")\n",
    "                #TODO FdH: implement unsafety probs (0.0, 0.0}\n",
    "        return Qn\n",
    "    assert Qn.shape == (n_states+2, 7**3)\n",
    "    assert safety_map is not None or unsafety_prob == 0.0\n",
    "    for n in range(n_epochs):\n",
    "        Qn = epoch(snsasr, Qn, gamma, learning_rate, unsafety_prob, safety_map)\n",
    "        assert np.nanmax(Qn) < 100, \"Scores > 100 should not occur, found: {}\".format(np.nanargmax(Qn))\n",
    "        print('.', end='')\n",
    "    return Qn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d35aae",
   "metadata": {},
   "source": [
    "## Q learning\n",
    "Derive a table of Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f6fcfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# learn a q table\n",
    "q_init_val = 0\n",
    "q_init = np.full((n_states + 2, 7**3), float(q_init_val))\n",
    "#  peine_mc_iterate(snsas, r, Qn, gamma, n_epochs=1, learning_rate=0.1):\n",
    "q_mcp = peine_mc_iterate(\n",
    "    # TODO: why are the NaNs here? how to deal with these?\n",
    "    snsasr=train_set[['state', 'next_state', 'action_discrete', 'reward']].astype(int).to_numpy(),\n",
    "    Qn=q_init,\n",
    "    gamma=GAMMA,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    unsafety_prob=UNSAFETY_PROB,\n",
    "    safety_map=safety.action_id_compliance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8587474f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/peine_mc_none_1.0_1.0_q_table_4.bin']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# postprocess q table -- removal of nans\n",
    "q_mcp_nan = q_mcp.copy()[:n_states, :]\n",
    "q_mcp_nan[q_mcp_nan == 0.0] = np.nan\n",
    "joblib.dump(\n",
    "    {'hyperparameters': {\n",
    "        'Q_init': q_init,\n",
    "        'gamma': GAMMA,\n",
    "        'n_epochs': N_EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'shaping': SHAPING,\n",
    "        'shaping_scalar': COMPLIANCE_SCALAR,\n",
    "        'unsafety_prob':UNSAFETY_PROB,\n",
    "        'softmax_temp': SOFTMAX_TEMPERATURE,\n",
    "        'safety_map':safety.action_id_compliance            \n",
    "    },\n",
    "    'model': q_mcp_nan,\n",
    "    },\n",
    "    'models/peine_mc_{}_{}_{}_q_table_{}.bin'.format(SHAPING, UNSAFETY_PROB, SOFTMAX_TEMPERATURE, seed),\n",
    "    compress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17bedf",
   "metadata": {},
   "source": [
    "## Policy learning\n",
    "Derive a policy from a table of Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bbcd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive policy by taking argmax over non-nan q values\n",
    "q_mcp_nan[np.isnan(q_mcp_nan).all(axis=1),:] = 0\n",
    "\n",
    "best_action_indices = np.nanargmax(q_mcp_nan, axis=1)\n",
    "\n",
    "action_index_grid = np.tile(np.array(range(7**3)), 650).reshape((650, 7**3))\n",
    "best_action_grid = np.repeat(best_action_indices, 7**3).reshape((650, 7**3))\n",
    "best_action_bool = best_action_grid == action_index_grid\n",
    "assert best_action_bool.shape == (n_states, 7**3)\n",
    "assert (best_action_bool.sum(axis=1) == 1).all()\n",
    "mcp_greedy = best_action_bool.astype(float)\n",
    "assert (mcp_greedy.sum(axis=1) == 1).all()\n",
    "\n",
    "# derive policy by taking softmax\n",
    "q_mcp_neg = q_mcp.copy()[:n_states, :]\n",
    "q_mcp_neg[q_mcp_neg == 0.0] = float('-inf')\n",
    "mcp_softmax = scipy.special.softmax(q_mcp_neg / SOFTMAX_TEMPERATURE, axis=1)\n",
    "assert mcp_softmax.shape == (n_states, 7**3)\n",
    "assert (mcp_greedy.sum(axis=1) == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680fdc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global highest Q value 95.77106122060216 for tv, fio2, peep ranges: ((10, 12.5), (40, 45), (5, 7))\n",
      "Highest avg Q value across states 63.49260873918384 for tv, fio2, peep ranges: ((7.5, 10), (50, 55), (5, 7))\n",
      "Highest median Q value across states 183 for tv, fio2, peep ranges: ((7.5, 10), (50, 55), (5, 7))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_461569/3524730465.py:4: RuntimeWarning: Mean of empty slice\n",
      "  best_mean_a, best_mean_a_q = np.nanargmax(np.nanmean(q_mcp_nan, axis=0)), np.nanmax(np.nanmean(q_mcp_nan, axis=0))\n",
      "/home/floris/anaconda3/envs/ventrl/lib/python3.11/site-packages/numpy/lib/nanfunctions.py:1095: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n"
     ]
    }
   ],
   "source": [
    "# some diagnostics\n",
    "best_s, best_a = np.unravel_index(np.nanargmax(q_mcp_nan), (n_states, 7**3))\n",
    "print(\"Global highest Q value {} for tv, fio2, peep ranges: {}\".format(q_mcp_nan[best_s, best_a], utils.to_action_ranges(best_a)))\n",
    "best_mean_a, best_mean_a_q = np.nanargmax(np.nanmean(q_mcp_nan, axis=0)), np.nanmax(np.nanmean(q_mcp_nan, axis=0))\n",
    "print(\"Highest avg Q value across states {} for tv, fio2, peep ranges: {}\".format(best_mean_a_q, utils.to_action_ranges(best_mean_a)))\n",
    "best_med_a, best_med_a_q = np.nanargmax(np.nanmedian(q_mcp_nan, axis=0)), np.nanmax(np.nanmedian(q_mcp_nan, axis=0))\n",
    "print(\"Highest median Q value across states {} for tv, fio2, peep ranges: {}\".format(best_med_a, utils.to_action_ranges(best_med_a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6016e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortnan(x, index):\n",
    "    return float('-inf') if np.isnan(x[index]) else x[index]\n",
    "\n",
    "if plot:\n",
    "    sns.histplot(q_mcp_nan[mcp_greedy == 1.0].ravel(), log_scale=(False, True), bins=200)\n",
    "    plt.xlabel('Q value')\n",
    "    plt.title('Histogram of Q values greedy policy')\n",
    "    plt.show()\n",
    "    train_set['positive_outcome'] = train_set['mort90day'] == 'f'\n",
    "    estimated_mort_state_visit = train_set.groupby('state').mean('positive_outcome')[['positive_outcome']].to_numpy()\n",
    "    sns.scatterplot(x=np.nanmean(q_mcp_nan, axis=1), y=estimated_mort_state_visit.reshape(n_states,))\n",
    "    plt.xlabel('Mean estimated Q value')\n",
    "    plt.ylabel('Average outcome')\n",
    "    plt.title('Outcome vs mean Q value estimates')\n",
    "    plt.show()\n",
    "    sns.scatterplot(x=np.nanmax(q_mcp_nan, axis=1), y=estimated_mort_state_visit.reshape(n_states,))\n",
    "    plt.xlabel('Max estimated Q value')\n",
    "    plt.ylabel('Average outcome')\n",
    "    plt.title('Outcome vs max Q value estimates')\n",
    "    plt.show()\n",
    "    sns.scatterplot(x=np.nanmedian(q_mcp_nan, axis=1), y=estimated_mort_state_visit.reshape(n_states,))\n",
    "    plt.xlabel('Max estimated Q value')\n",
    "    plt.ylabel('Average outcome')\n",
    "    plt.title('Outcome vs median Q value estimates')\n",
    "    plt.show()\n",
    "    \n",
    "    q_vars = np.nanvar(q_mcp_nan, axis=1)\n",
    "    q_means = np.nanmean(q_mcp_nan, axis=1)\n",
    "    q_medians = np.nanmedian(q_mcp_nan, axis=1)\n",
    "    q_maxs = np.nanmax(q_mcp_nan, axis=1)\n",
    "    q_mins = np.nanmin(q_mcp_nan, axis=1)\n",
    "    stacked = np.column_stack((q_means, q_medians, q_maxs, q_mins, q_vars))\n",
    "    xs = range(n_states)\n",
    "    means_sorted = np.array(sorted(stacked, key=lambda x: x[0]))\n",
    "    means_upper = means_sorted[:, 0] + means_sorted[:, -1]\n",
    "    means_lower = means_sorted[:, 0] - means_sorted[:, -1]\n",
    "    axs = sns.lineplot(x=xs, y=means_sorted[:, 0])\n",
    "    axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "    axs.set_ylim(-150, 150)\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Mean Q values per state +- 1 var')\n",
    "    plt.show()\n",
    "\n",
    "    medians_sorted = np.array(sorted(stacked, key=lambda x: x[1]))\n",
    "    means_upper = medians_sorted[:, 0] + medians_sorted[:, -1]\n",
    "    means_lower = medians_sorted[:, 0] - medians_sorted[:, -1]\n",
    "    axs = sns.lineplot(x=xs, y=medians_sorted[:, 1])\n",
    "    axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "    axs.set_ylim(-150, 150)\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Median Q values per state +- 1 var')\n",
    "    plt.show()\n",
    "\n",
    "    mins_sorted = np.array(sorted(stacked, key=lambda x: x[3]))\n",
    "    axs = sns.scatterplot(x=xs, y=mins_sorted[:, 3], color='orange', alpha=.5)\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Min Q values per state')\n",
    "\n",
    "    maxs_sorted = np.array(sorted(stacked, key=lambda x: x[2]))\n",
    "    axs = sns.lineplot(x=xs, y=maxs_sorted[:, 2], alpha=.5)\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Max Q values per state')\n",
    "    plt.show()\n",
    "\n",
    "    q_vars = np.nanvar(q_mcp_nan, axis=0)\n",
    "    q_means = np.nanmean(q_mcp_nan, axis=0)\n",
    "    q_medians = np.nanmedian(q_mcp_nan, axis=0)\n",
    "    q_maxs = np.nanmax(q_mcp_nan, axis=0)\n",
    "    q_mins = np.nanmin(q_mcp_nan, axis=0)\n",
    "    stacked = np.column_stack((q_means, q_medians, q_maxs, q_mins, q_vars))\n",
    "    xs = range(7**3)\n",
    "    means_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 0)))\n",
    "    means_upper = means_sorted[:, 0] + means_sorted[:, -1]\n",
    "    means_lower = means_sorted[:, 0] - means_sorted[:, -1]\n",
    "    axs = sns.lineplot(x=xs, y=means_sorted[:, 0])\n",
    "    axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "    axs.set_ylim(-150, 150)\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Mean Q values per action +- 1 var')\n",
    "    plt.show()\n",
    "\n",
    "    medians_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 1)))\n",
    "    means_upper = medians_sorted[:, 0] + medians_sorted[:, -1]\n",
    "    means_lower = medians_sorted[:, 0] - medians_sorted[:, -1]\n",
    "    axs = sns.lineplot(x=xs, y=medians_sorted[:, 1])\n",
    "    axs.fill_between(x=xs, y1=means_lower, y2=means_upper, alpha=.3)\n",
    "    axs.set_ylim(-150, 150)\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Median Q values per action +- 1 var')\n",
    "    plt.show()\n",
    "\n",
    "    maxs_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 2)))\n",
    "    axs = sns.lineplot(x=xs, y=maxs_sorted[:, 2])\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Max Q values per action')\n",
    "    plt.show()\n",
    "\n",
    "    mins_sorted = np.array(sorted(stacked, key=lambda x: sortnan(x, 3)))\n",
    "    axs = sns.lineplot(x=xs, y=mins_sorted[:, 3])\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Q value')\n",
    "    plt.title('Min Q values per action')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2cf2a",
   "metadata": {},
   "source": [
    "## Derive behavior policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f297f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy train-test behavior policy: 9.032339475077848\n",
      "Behavior policy argmax and greedy policy agreement: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n",
      "/tmp/ipykernel_461569/2838543815.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  behavior_policy_pivot.loc[:, a] = np.nan\n"
     ]
    }
   ],
   "source": [
    "if UNSAFETY_PROB == 1.0: # only derive behavior policy if all actions are allowed\n",
    "    # Train set\n",
    "    behavior_policy_df = (test_set.value_counts(['state', 'action_discrete']) / test_set.value_counts(['state']))\n",
    "    assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "    behavior_policy_df = behavior_policy_df.reset_index()\n",
    "\n",
    "    behavior_policy_df = train_set.value_counts(['state', 'action_discrete']) / train_set.value_counts(['state'])\n",
    "    assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "\n",
    "    behavior_policy_pivot = behavior_policy_df.reset_index().pivot(columns='action_discrete', index='state')[0]\n",
    "    behavior_policy_states = set(behavior_policy_pivot.index.unique())\n",
    "    for s in range(n_states):\n",
    "        if s not in behavior_policy_states:\n",
    "            action_probs = [1.0 / (7**3),] * 7**3 # uniform distribution\n",
    "            for i, p in enumerate(action_probs):\n",
    "                behavior_policy_pivot.loc[s] = [s, i, p]\n",
    "\n",
    "    behavior_policy_pivot = behavior_policy_pivot.sort_values(['state'])\n",
    "\n",
    "    for a in range(7**3):\n",
    "        if a not in behavior_policy_pivot.columns:\n",
    "            behavior_policy_pivot.loc[:, a] = np.nan\n",
    "\n",
    "    behavior_policy_nan = behavior_policy_pivot[range(7**3)].to_numpy()\n",
    "    assert (1- (np.nansum(behavior_policy_nan, axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "    behavior_policy = np.nan_to_num(behavior_policy_nan, 0.0)\n",
    "    assert (1- (behavior_policy.sum(axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "    assert behavior_policy.shape == (n_states, 7**3), \"Behavior policy should cover all states and actions\"\n",
    "    mcp_greedy_mask = mcp_greedy.astype(bool)\n",
    "    assert (mcp_greedy_mask.sum(axis=1) == 1).all(), \"Greedy policy mask should mask out all-but-one action\"\n",
    "    joblib.dump(behavior_policy, \"models/clinicians_policy_train_{}.bin\".format(seed), compress=True)\n",
    "    \n",
    "    # Test set\n",
    "    behavior_policy_df = test_set.value_counts(['state', 'action_discrete']) / test_set.value_counts(['state'])\n",
    "    assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "\n",
    "    behavior_policy_pivot = behavior_policy_df.reset_index().pivot(columns='action_discrete', index='state')[0]\n",
    "\n",
    "    for a in range(7**3):\n",
    "        if a not in behavior_policy_pivot.columns:\n",
    "            behavior_policy_pivot.loc[:, a] = np.nan\n",
    "    behavior_policy_states = set(behavior_policy_pivot.index.unique())\n",
    "    for s in range(n_states):\n",
    "        if s not in behavior_policy_states:\n",
    "            action_probs = [1.0 / (7**3),] * 7**3 # uniform distribution\n",
    "            for i, p in enumerate(action_probs):\n",
    "                behavior_policy_pivot.loc[s] = [1/(7**3),]*(7**3)\n",
    "\n",
    "    behavior_policy_pivot = behavior_policy_pivot.sort_values(['state'])\n",
    "\n",
    "    # train + test set\n",
    "    behavior_policy_nan = behavior_policy_pivot[range(7**3)].to_numpy()\n",
    "    assert (1- (np.nansum(behavior_policy_nan, axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "    behavior_policy = np.nan_to_num(behavior_policy_nan, 0.0)\n",
    "    assert (1- (behavior_policy.sum(axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "    assert behavior_policy.shape == (n_states, 7**3), \"Behavior policy should cover all states and actions\"\n",
    "    mcp_greedy_mask = mcp_greedy.astype(bool)\n",
    "    assert (mcp_greedy_mask.sum(axis=1) == 1).all(), \"Greedy policy mask should mask out all-but-one action\"\n",
    "    joblib.dump(behavior_policy, \"models/clinicians_policy_test_{}.bin\".format(seed), compress=True)\n",
    "    \n",
    "    train_test = pd.concat([train_set, test_set])\n",
    "    behavior_policy_df = train_test.value_counts(['state', 'action_discrete']) / train_test.value_counts(['state'])\n",
    "    assert (1.0 - behavior_policy_df.groupby('state').sum() < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "\n",
    "    behavior_policy_pivot = behavior_policy_df.reset_index().pivot(columns='action_discrete', index='state')[0]\n",
    "    for a in range(7**3):\n",
    "        if a not in behavior_policy_pivot.columns:\n",
    "            behavior_policy_pivot.loc[:, a] = np.nan\n",
    "\n",
    "    behavior_policy_nan = behavior_policy_pivot[range(7**3)].to_numpy()\n",
    "    assert (1- (np.nansum(behavior_policy_nan, axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "    behavior_policy = np.nan_to_num(behavior_policy_nan, 0.0)\n",
    "    assert (1- (behavior_policy.sum(axis=1)) < 1e10).all(), \"Policy action probs should sum to 1 per state\"\n",
    "    assert behavior_policy.shape == (n_states, 7**3), \"Behavior policy should cover all states and actions\"\n",
    "    mcp_greedy_mask = mcp_greedy.astype(bool)\n",
    "    assert (mcp_greedy_mask.sum(axis=1) == 1).all(), \"Greedy policy mask should mask out all-but-one action\"\n",
    "    joblib.dump(behavior_policy, \"models/clinicians_policy_train_test_{}.bin\".format(seed), compress=True)\n",
    "    print(\"Entropy train-test behavior policy: {}\".format(scipy.stats.entropy(behavior_policy.ravel())))\n",
    "    print(\"Behavior policy argmax and greedy policy agreement: {}\".format((behavior_policy.argmax(axis=1) == mcp_greedy.argmax(axis=1)).sum() / n_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5161907",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot and UNSAFETY_PROB == 1.0:\n",
    "    sns.histplot(scipy.stats.entropy(behavior_policy, axis=1))\n",
    "    plt.title('Behavior policy per-state entropy')\n",
    "    plt.xlabel('Entropy') \n",
    "    plt.show()\n",
    "    \n",
    "    sns.histplot(scipy.stats.entropy(mcp_softmax, axis=1))\n",
    "    plt.title('Softmax policy per-state entropy')\n",
    "    plt.xlabel('Entropy')\n",
    "    plt.show()\n",
    "    \n",
    "    sns.histplot(behavior_policy[mcp_greedy_mask], log_scale=(False, True))\n",
    "    plt.xlabel('Action probability greedy policy in behavior policy')\n",
    "    behavior_policy[mcp_greedy_mask].min(), behavior_policy[mcp_greedy_mask].max()\n",
    "    plt.show()\n",
    "    \n",
    "    evaluation_policy = mcp_greedy\n",
    "\n",
    "    behavior_policy_ranks = np.flip(behavior_policy.argsort(axis=1), axis=1)\n",
    "    ep_bp_ranks = []\n",
    "    for s in range(n_states):\n",
    "        ep_a = evaluation_policy[s,:].argmax()\n",
    "        bp_rank = np.where(behavior_policy_ranks[s, :] == ep_a)[0][0]\n",
    "        ep_bp_ranks.append(bp_rank)\n",
    "\n",
    "    sns.histplot(ep_bp_ranks, bins=60)\n",
    "    plt.title('Greedy policy action ranks in behavior policy')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.show()\n",
    "\n",
    "    behavior_policy_ranked_probs = np.flip(np.sort(behavior_policy, axis=1), axis=1)\n",
    "    ep_bp_prob_mass = []\n",
    "    for s in range(n_states):\n",
    "        ep_a = evaluation_policy[s,:].argmax()\n",
    "        bp_rank = np.where(behavior_policy_ranks[s, :] == ep_a)[0][0]\n",
    "        ep_bp_prob_mass.append(behavior_policy_ranked_probs[s, 0:bp_rank].sum())\n",
    "\n",
    "    sns.histplot(ep_bp_prob_mass)\n",
    "    plt.title('Probability mass up to greedy actduion')\n",
    "    plt.xlabel('Action probs')\n",
    "    plt.show()\n",
    "    np.array(ep_bp_prob_mass).min(), np.array(ep_bp_prob_mass).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95508d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/mcp_softmax_policy_4_none_0.0_1.0.bin']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mcp_greedy, \"models/mcp_greedy_policy_{}_{}_{}_{}.bin\".format(seed, SHAPING, COMPLIANCE_SCALAR, UNSAFETY_PROB), compress=True)\n",
    "joblib.dump(mcp_softmax, \"models/mcp_softmax_policy_{}_{}_{}_{}.bin\".format(seed, SHAPING, COMPLIANCE_SCALAR, UNSAFETY_PROB), compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbce1a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: 4 none 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"done:\", seed, SHAPING, COMPLIANCE_SCALAR, UNSAFETY_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9063a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
